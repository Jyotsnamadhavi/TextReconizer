{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzWKWfSTFrvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8Y7CwO4HOO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRWiM42aHQgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load MNIST datset which is internal \n",
        "digits = datasets.load_digits(n_class=1000)\n",
        "\n",
        "# separate our data and label\n",
        "data = digits.data\n",
        "target = digits.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nMUf1SKH56y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a636ea6e-8f4e-49a7-871d-8d664daf7041"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuB-2TQuH_dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvGX7r4BIHex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test =  train_test_split(data, target, test_size=0.2, random_state=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmJ45GWMIpwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to Create MLP classifier object with hyper parameters\n",
        "def mlp(a,s,h,lr):\n",
        "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,max_iter = 10000 ,learning_rate = 'constant',learning_rate_init=lr)\n",
        "    return clf \n",
        "  \n",
        "#function to calculate the accuracy\n",
        "def accuracy(actual,predicted):\n",
        "    return accuracy_score(actual,predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UElLvohoIwLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# activation: Activation functions are critical in introducing non-linearity in MLP (in absence of this all layers of MLP combine into a single layer)\n",
        "activation = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
        "#solvers: The following are the methods by which your weights get updated.\n",
        "solvers = [\"lbfgs\",\"sgd\",\"adam\"]\n",
        "#learning rate\n",
        "learning_rate = [0.0001,0.001,0.01,0.1]\n",
        "#hidden layers\n",
        "hidden_layers = [(4,2),(5,1),(6,3),(6,8),(2,3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diad-Db2JCq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b73da26-b676-4e95-db56-d529099b82af"
      },
      "source": [
        "test_accuracy = []\n",
        "validation_accuracy = []\n",
        "train_accuracy = []\n",
        "for i in range(10):\n",
        "    k1 = np.random.randint(0,len(activation))\n",
        "    k2 = np.random.randint(0,len(solvers))\n",
        "    k3 = np.random.randint(0,len(learning_rate))\n",
        "    k4 = np.random.randint(0,len(hidden_layers))\n",
        "    print(\"\\nHyper-parameters = \\n activation = \", activation[k1],    \"\\n solver = \", solvers[k2], \"\\n learning_rate_init = \", learning_rate[k3],         \"\\n hidden_layer_sizes = \", hidden_layers[k4])\n",
        "    #calling the mlp function with random hyper paramters\n",
        "    clf = mlp(activation[k1],solvers[k2],hidden_layers[k4],learning_rate[k3])\n",
        "    #Fitting the data into model\n",
        "    clf.fit(X_train,Y_train)\n",
        "    ## Predicting the values on trained model using train data\n",
        "    predTrain = clf.predict((X_train))\n",
        "    #Calculating the train accuracy\n",
        "    train_accuracy.append(accuracy(Y_train,predTrain))\n",
        "    # Predicting the values on trained model using test data\n",
        "    predTest = clf.predict((X_test))\n",
        "    #Calculating the test accuracy\n",
        "    test_accuracy.append(accuracy(Y_test,predTest))\n",
        "  \n",
        "    print(\"(train,  test) accuracy = \",accuracy(Y_train,predTrain),  accuracy(Y_test,predTest))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (2, 3)\n",
            "(train,  test) accuracy =  0.7926235212247739 0.7555555555555555\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (5, 1)\n",
            "(train,  test) accuracy =  0.1057759220598469 0.08333333333333333\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  adam \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (2, 3)\n",
            "(train,  test) accuracy =  0.6938065414057063 0.6916666666666667\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (5, 1)\n",
            "(train,  test) accuracy =  0.1057759220598469 0.08333333333333333\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (5, 1)\n",
            "(train,  test) accuracy =  0.3945720250521921 0.3333333333333333\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  adam \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (6, 3)\n",
            "(train,  test) accuracy =  0.27905358385525403 0.2972222222222222\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  adam \n",
            " learning_rate_init =  0.0001 \n",
            " hidden_layer_sizes =  (5, 1)\n",
            "(train,  test) accuracy =  0.1057759220598469 0.08333333333333333\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  logistic \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.01 \n",
            " hidden_layer_sizes =  (5, 1)\n",
            "(train,  test) accuracy =  0.34794711203897005 0.30277777777777776\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  relu \n",
            " solver =  sgd \n",
            " learning_rate_init =  0.001 \n",
            " hidden_layer_sizes =  (5, 1)\n",
            "(train,  test) accuracy =  0.4328462073764788 0.33611111111111114\n",
            "\n",
            "Hyper-parameters = \n",
            " activation =  identity \n",
            " solver =  lbfgs \n",
            " learning_rate_init =  0.1 \n",
            " hidden_layer_sizes =  (6, 8)\n",
            "(train,  test) accuracy =  1.0 0.9472222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67v9mUkHJuM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}